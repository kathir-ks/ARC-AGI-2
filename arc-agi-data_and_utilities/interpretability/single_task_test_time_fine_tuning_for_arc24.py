# -*- coding: utf-8 -*-
"""Single-task Test-time fine-tuning for ARC24

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/#fileId=https%3A//storage.googleapis.com/kaggle-colab-exported-notebooks/kathirksw/single-task-test-time-fine-tuning-for-arc24.3105b3ab-7e69-4b64-b54a-fad880f08ff0.ipynb%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com/20250816/auto/storage/goog4_request%26X-Goog-Date%3D20250816T153738Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D3c237c93803e623797bbe3cb9999b9db3092480ba673014c390fe7b391760f56df82331269b7b35ea98d473a5ac7bdc6aa7ca557b9f59eb4256ab210c9e065bfd9eab297ef04fe2916cf5abb3a528ac3a2fd598a2ac6122b67c3ad5805ad846a7a46708413286a93df90506883b97810c5cdd35540bdff2b286aaab0aee97fd6d9dba765c524339a11e3b413c8a0aa847ef509bb24dd2376cea9d93716187457faa59c50aa40e94da8723c7a22ce9ec568d972500d1ea047e80329717a3ecfa910e70d6d88c7a8e39f27b8c49b556e14cf741dfb3a7eef492da5d63e676ce5cdc207fea93c1198ddb0b7f724f08cb303d3a88038ff5c3d4929a4ce4167606b71
"""

# IMPORTANT: SOME KAGGLE DATA SOURCES ARE PRIVATE
# RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES.
import kagglehub
kagglehub.login()

# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES,
# THEN FEEL FREE TO DELETE THIS CELL.
# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON
# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR
# NOTEBOOK.

arc_prize_2024_path = kagglehub.competition_download('arc-prize-2024')
hansuelijud_arc_solution_source_files_by_icecuber_path = kagglehub.dataset_download('hansuelijud/arc-solution-source-files-by-icecuber')
ironbar_arc24_source_code_path = kagglehub.dataset_download('ironbar/arc24-source-code')
ironbar_making_wheels_of_necessary_packages_for_vllm_path = kagglehub.notebook_output_download('ironbar/making-wheels-of-necessary-packages-for-vllm')
marquis03_qwen2_transformers_qwen2_0_5b_instruct_1_path = kagglehub.model_download('marquis03/qwen2/Transformers/qwen2-0.5b-instruct/1')
marquis03_qwen2_transformers_qwen2_1_5b_instruct_1_path = kagglehub.model_download('marquis03/qwen2/Transformers/qwen2-1.5b-instruct/1')
ironbar_loras_transformers_qwen2_0_5b_instruct_4_path = kagglehub.model_download('ironbar/loras/Transformers/qwen2-0.5b-instruct/4')
ironbar_loras_transformers_qwen2_0_5b_instruct_5_path = kagglehub.model_download('ironbar/loras/Transformers/qwen2-0.5b-instruct/5')
ironbar_loras_transformers_qwen2_0_5b_instruct_6_path = kagglehub.model_download('ironbar/loras/Transformers/qwen2-0.5b-instruct/6')
ironbar_loras_transformers_qwen2_0_5b_instruct_7_path = kagglehub.model_download('ironbar/loras/Transformers/qwen2-0.5b-instruct/7')
ironbar_loras_transformers_qwen2_0_5b_instruct_8_path = kagglehub.model_download('ironbar/loras/Transformers/qwen2-0.5b-instruct/8')
ironbar_loras_transformers_qwen2_0_5b_instruct_9_path = kagglehub.model_download('ironbar/loras/Transformers/qwen2-0.5b-instruct/9')
ironbar_loras_transformers_qwen2_0_5b_instruct_10_path = kagglehub.model_download('ironbar/loras/Transformers/qwen2-0.5b-instruct/10')
ironbar_loras_transformers_qwen2_0_5b_instruct_11_path = kagglehub.model_download('ironbar/loras/Transformers/qwen2-0.5b-instruct/11')
ironbar_loras_transformers_qwen2_0_5b_instruct_12_path = kagglehub.model_download('ironbar/loras/Transformers/qwen2-0.5b-instruct/12')
ironbar_loras_transformers_qwen2_0_5b_instruct_13_path = kagglehub.model_download('ironbar/loras/Transformers/qwen2-0.5b-instruct/13')
ironbar_loras_transformers_qwen2_0_5b_instruct_14_path = kagglehub.model_download('ironbar/loras/Transformers/qwen2-0.5b-instruct/14')
ironbar_loras_transformers_qwen2_0_5b_instruct_15_path = kagglehub.model_download('ironbar/loras/Transformers/qwen2-0.5b-instruct/15')
ironbar_loras_transformers_qwen2_0_5b_instruct_17_path = kagglehub.model_download('ironbar/loras/Transformers/qwen2-0.5b-instruct/17')
ironbar_loras_transformers_qwen2_0_5b_instruct_18_path = kagglehub.model_download('ironbar/loras/Transformers/qwen2-0.5b-instruct/18')
ironbar_loras_transformers_qwen2_0_5b_instruct_19_path = kagglehub.model_download('ironbar/loras/Transformers/qwen2-0.5b-instruct/19')
ironbar_qwen2_5_transformers_0_5b_instruct_1_path = kagglehub.model_download('ironbar/qwen2.5/Transformers/0.5b-instruct/1')
ironbar_loras_transformers_qwen2_5_0_5b_instruct_1_path = kagglehub.model_download('ironbar/loras/Transformers/qwen2.5-0.5b-instruct/1')
ironbar_loras_transformers_qwen2_5_0_5b_instruct_2_path = kagglehub.model_download('ironbar/loras/Transformers/qwen2.5-0.5b-instruct/2')
ironbar_loras_transformers_qwen2_5_0_5b_instruct_3_path = kagglehub.model_download('ironbar/loras/Transformers/qwen2.5-0.5b-instruct/3')
ironbar_loras_transformers_qwen2_5_0_5b_instruct_4_path = kagglehub.model_download('ironbar/loras/Transformers/qwen2.5-0.5b-instruct/4')
ironbar_loras_transformers_qwen2_5_0_5b_instruct_5_path = kagglehub.model_download('ironbar/loras/Transformers/qwen2.5-0.5b-instruct/5')
ironbar_loras_transformers_qwen2_5_0_5b_instruct_6_path = kagglehub.model_download('ironbar/loras/Transformers/qwen2.5-0.5b-instruct/6')
ironbar_loras_transformers_qwen2_5_0_5b_instruct_7_path = kagglehub.model_download('ironbar/loras/Transformers/qwen2.5-0.5b-instruct/7')
ironbar_loras_transformers_qwen2_5_0_5b_instruct_8_path = kagglehub.model_download('ironbar/loras/Transformers/qwen2.5-0.5b-instruct/8')
ironbar_loras_transformers_qwen2_5_0_5b_instruct_9_path = kagglehub.model_download('ironbar/loras/Transformers/qwen2.5-0.5b-instruct/9')

print('Data source import complete.')

"""# Single-task Test-time fine-tuning for ARC24

## Goal

In this notebook I will explore a version of test-time fine-tuning that adapts the base model for each task.

Instead of fine-tuning in all the test tasks together, I will fine-tune a model for each of the tasks. Hopefully this will bring the improvement that Jack Cole is talking about in the MLST podcast.

## Configuration and imports
"""

!jq 'to_entries | .[:2] | from_entries' /kaggle/input/arc24-source-code/new_partitions/val_rs7.json > smaller_val_challenges.json

#qwen25-0.5b/8 100split step32k_bs1 8e-5lr_lin 96E
n_splits = 100 #2, 4, 10, 20, 50, 100
total_train_steps = 32000
class cfg:
    # Model
    model_path = '/kaggle/input/qwen2.5/transformers/0.5b-instruct/1'
    input_lora_path = '/kaggle/input/loras/transformers/qwen2.5-0.5b-instruct/8'
    prompt_version = 'output-from-examples-v1'
    merged_model_path = '/kaggle/tmp/qwen_merged_model'
    grid_encoder = 'GridShapeEncoder(RowNumberEncoder(MinimalGridEncoder()))' #GridCodeBlockEncoder(MinimalGridEncoder())
    max_model_len = 10240
    # Dataset
    dataset_path = '/kaggle/input/arc-prize-2024/arc-agi_test_challenges.json'
    #dataset_path = '/kaggle/input/arc24-source-code/new_partitions/val_rs7.json'
    #dataset_path = 'smaller_val_challenges.json'
    split_size = 100//n_splits # How many tasks there would be on each split, use 1 for the canonical single-task test-time fine-tuning
    # Fine-tuning params
    max_steps = total_train_steps//n_splits
    learning_rate = 8e-5 #1e-4 for lora smaller than 512
    lr_scheduler_type: str = "linear" #linear, constant_with_warmup, cosine, cosine_with_restarts
    batch_size = 1
    max_seq_len = 5120 #3456, 5120
    # Inference params
    predictions_per_task = 96 # multiple of 8
    inference_timeout = "12m" # max inference time per split, I estimate that for 128 preds Qwen-0.5B takes 6 min in the worst case (50 splits)
    # Ensemble
    ensemble_with_2020: bool = True

import os
is_dry_run = cfg.dataset_path == '/kaggle/input/arc-prize-2024/arc-agi_test_challenges.json' and not os.getenv('KAGGLE_IS_COMPETITION_RERUN')
if is_dry_run:
    print('This is a dry run, no inference nor installation of packages will be done')

if int(cfg.input_lora_path.split('/')[-1]) < 18 and cfg.input_lora_path.startswith('/kaggle/input/loras/transformers/qwen2-0.5b'):
    assert cfg.prompt_version == 'output-from-examples-v0'
else:
    assert cfg.prompt_version == 'output-from-examples-v1'

import logging
import subprocess
import sys
import json
import glob
import os
import shutil
from tqdm.auto import tqdm

if not is_dry_run:
    sys.path.append('/kaggle/input/arc24-source-code')

# Configure logging to output to the notebook console
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s', force=True)

"""## 0. Launch 2020 solution in the background"""

if not is_dry_run and cfg.ensemble_with_2020:
    print('Launching 2020 solution in the background')
    args = [
        #'taskset', '-c', '0', apparently this will restrict the job to a single cpu
        'python',
        '/kaggle/input/arc24-source-code/full_2020_solution.py',
        f'--dataset_filepath={cfg.dataset_path}',
        '--icecuber_output_filepath=icecuber_submission.json',
        '--dsl_output_filepath=submission_program_search.json']
    full_2020_solution_process = subprocess.Popen(args, stdout=subprocess.PIPE, stderr=subprocess.PIPE)

"""## 1. Install libraries and launch resource monitor"""

# Commented out IPython magic to ensure Python compatibility.
# %%time
# if not is_dry_run:
#     !bash /kaggle/input/arc24-source-code/install_libraries.sh

if not is_dry_run:
    from arc24.utils import ResourceMonitor
    monitor = ResourceMonitor(interval=1)
    monitor.start()

"""## 2. Prepare data for training"""

if not is_dry_run:
    single_task_datasets_path = 'single_task_datasets'
    os.makedirs(single_task_datasets_path, exist_ok=True)
    with open(cfg.dataset_path, 'r') as f:
        items = list(json.load(f).items())

    assert len(items) % cfg.split_size == 0

    for batch_idx in tqdm(range(len(items)//cfg.split_size), desc='Creating single task datasets'):
        data = dict(items[batch_idx*cfg.split_size: (batch_idx + 1)*cfg.split_size])
        assert len(data) == cfg.split_size
        task_id = list(data.keys())[0]
        with open(os.path.join(single_task_datasets_path, f'{task_id}.json'), 'w') as f:
            json.dump(data, f)
    ! ls -lh {single_task_datasets_path}

if not is_dry_run:
    training_datasets_path = 'single_task_training_datasets'
    os.makedirs(training_datasets_path, exist_ok=True)
    dataset_filepaths = glob.glob(os.path.join(single_task_datasets_path, '*.json'))
    for dataset_filepath in tqdm(dataset_filepaths, desc='Creating ttft training datasets'):
        !python /kaggle/input/arc24-source-code/create_n-1_dataset.py \
        {dataset_filepath} \
        {os.path.join(training_datasets_path, os.path.basename(dataset_filepath))}

"""## 3. Test-time fine-tuning"""

# Commented out IPython magic to ensure Python compatibility.
# %%time
# def clean_train_output_except_adapter(output_dir):
#     """
#     Max Disk is 57.6GiB, but around 8GiB are already used.
#     If each checkpoint weights 265M, that would be 26GB for 100 savings. The optimizer was using just 133M.
#     However on '/kaggle/working/' we can only save 20GB, the run that had the disk error used 50 splits, each split was using around 400M, so it makes sense
#     """
#     #!rm -rf {output_dir}/runs
#     !rm {output_dir}/*/*.pth {output_dir}/*/*.pt {output_dir}/*/*.md {output_dir}/*/*.txt {output_dir}/*/*.bin {output_dir}/*/token*
#     !rm {output_dir}/*/added_tokens.json {output_dir}/*/special_tokens_map.json {output_dir}/*/vocab.json {output_dir}/*/trainer_state.json
# 
# if not is_dry_run:
#     dataset_filepaths = sorted(glob.glob(os.path.join(training_datasets_path, '*.json')))
#     checkpoints_folder = '/kaggle/tmp/checkpoints' # https://www.kaggle.com/docs/notebooks#technical-specifications
#     os.makedirs(checkpoints_folder, exist_ok=True)
#     for dataset_filepath in tqdm(dataset_filepaths, desc='Finetuning models'):
#         output_dir = os.path.join(checkpoints_folder, os.path.splitext(os.path.basename(dataset_filepath))[0])
#         !python /kaggle/input/arc24-source-code/fine-tuning.py \
#         --model_path={cfg.model_path} \
#         --adapter_path={cfg.input_lora_path} \
#         --output_dir={output_dir} \
#         --train_datasets {dataset_filepath} {cfg.prompt_version} \
#         --val_dataset {dataset_filepath} {cfg.prompt_version} \
#         --max_steps={cfg.max_steps} \
#         --eval_steps={cfg.max_steps*2} \
#         --max_seq_len={cfg.max_seq_len} \
#         --learning_rate={cfg.learning_rate} \
#         --lr_scheduler_type={cfg.lr_scheduler_type} \
#         --batch_size={cfg.batch_size} \
#         --report_to=tensorboard \
#         --grid_encoder="{cfg.grid_encoder}" \
#         --remove_train_samples_to_fit_max_seq_len \
#         --torch_dtype=float16 \
#         --no-verbose
#         clean_train_output_except_adapter(output_dir)
#         logging.info(f'Finished fine-tuning for split {dataset_filepaths.index(dataset_filepath) + 1}/{len(dataset_filepaths)}')

!ls -lh {checkpoints_folder}/*/checkpoint*/adapter_model.safetensors

"""## 5. Program search

https://www.kaggle.com/code/mehrankazeminia/3-arc24-developed-2020-winning-solutions

I'm doing program search before inference because I have observed OOM errors when running icecuber solution and inference in parallel. It seems I can fine-tune and run icecuber solution at the same time without any problem.
"""

if not is_dry_run and cfg.ensemble_with_2020:
    # old code to call program search dsl sequentially
    #!python /kaggle/input/arc24-source-code/program_search_dsl.py \
    #--dataset_filepath={cfg.dataset_path} \
    #--output_filepath=submission_program_search.json

    print('Waiting for icecuber process to end')
    full_2020_solution_process.wait()
    stdout, stderr = full_2020_solution_process.communicate()
    print("Script output:", stdout.decode())
    print("Script errors:", stderr.decode())

    #old code to call icecuber solution sequentially
    #!python /kaggle/input/arc24-source-code/icecuber_solution.py \
    #--dataset_filepath={cfg.dataset_path} \
    #--output_filepath=icecuber_submission.json

"""## 6. Inference"""

# Commented out IPython magic to ensure Python compatibility.
# %%time
# if is_dry_run:
#     with open('submission.json', 'w') as f:
#         json.dump(dict(dry_run=True), f)
# else:
#     inference_path = 'inference'
#     os.makedirs(inference_path)
#     os.environ['VLLM_LOGGING_LEVEL'] = 'ERROR'
#     dataset_filepaths = sorted(glob.glob(os.path.join(single_task_datasets_path, '*.json')))
#     for dataset_filepath in tqdm(dataset_filepaths):
#         task_id = os.path.splitext(os.path.basename(dataset_filepath))[0]
#         checkpoint_path = os.path.join(checkpoints_folder, task_id, f'checkpoint-{cfg.max_steps}')
#         if not os.path.exists(checkpoint_path):
#             print(f'Checkpoint path does not exist: {checkpoint_path}')
#             checkpoint_path = cfg.input_lora_path
# 
#         !python /kaggle/input/arc24-source-code/merge_lora.py \
#         --base_model_path={cfg.model_path} \
#         --lora_path={checkpoint_path} \
#         --output_path={cfg.merged_model_path}
# 
#         output_filepath = os.path.join(inference_path, f'{task_id}_inference.json')
#         while not os.path.exists(output_filepath):
#             ! timeout {cfg.inference_timeout} python /kaggle/input/arc24-source-code/inference.py\
#             --model_path={cfg.merged_model_path} \
#             --prompt_version={cfg.prompt_version} \
#             --dataset={dataset_filepath} \
#             --output_filepath={output_filepath} \
#             --max_model_len={cfg.max_model_len} \
#             --grid_encoder="{cfg.grid_encoder}" \
#             --predictions_per_task={cfg.predictions_per_task}
#             if not os.path.exists(output_filepath):
#                 print('\t\tWARNING, INFERENCE DID TIMEOUT!')
# 
#         logging.info(f'Finished inference for split {dataset_filepaths.index(dataset_filepath) + 1}/{len(dataset_filepaths)}')

# combine all the predictions into single files
if not is_dry_run:
    filepaths = glob.glob(os.path.join(inference_path, '*_inference.json'))
    solutions = dict()
    for filepath in tqdm(filepaths):
        with open(filepath, 'r') as f:
            solutions.update(json.load(f))
    with open('submission_all.json', 'w') as f:
        json.dump(solutions, f)

    filepaths = glob.glob(os.path.join(inference_path, '*_task_results.json'))
    task_results = []
    for filepath in tqdm(filepaths):
        with open(filepath, 'r') as f:
            task_results.extend(json.load(f))
    with open('submission_all_task_results.json', 'w') as f:
        json.dump(task_results, f)

if not is_dry_run:
    !python /kaggle/input/arc24-source-code/voting.py \
    --input_filepath=submission_all_task_results.json \
    --output_filepath=submission_voting.json

if not is_dry_run and cfg.dataset_path != '/kaggle/input/arc-prize-2024/arc-agi_test_challenges.json':
    sys.path.append('/kaggle/input/arc24-source-code')
    from evaluation import (
        load_arc_data_with_solutions, evaluate,
        study_effect_of_the_number_of_solutions,
        study_attempt_accuracy,
        visualize_tasks_and_predictions)

    print('Results with all the predictions')
    with open('submission_all.json', 'r') as f:
        solutions = json.load(f)
    data = load_arc_data_with_solutions(cfg.dataset_path)
    evaluate(data, solutions)

    study_effect_of_the_number_of_solutions(solutions, data)
    visualize_tasks_and_predictions(solutions, data, only_correct=False)

    print('Results from selected 2 attemps')
    with open('submission_voting.json', 'r') as f:
        solutions = json.load(f)
    evaluate(data, solutions)
    study_attempt_accuracy(solutions, data)

"""## 7. Combine solutions"""

if not is_dry_run:
    if cfg.ensemble_with_2020:
        !python /kaggle/input/arc24-source-code/combine_submissions.py \
        --sub_1=submission_program_search.json \
        --sub_2=icecuber_submission.json \
        --output=submission_2020.json \
        --give_preference_to_second_submission_on_second_attempt

        !python /kaggle/input/arc24-source-code/combine_submissions.py \
        --sub_1=submission_2020.json \
        --sub_2=submission_voting.json \
        --output=submission.json \
        --give_preference_to_second_submission_on_second_attempt
    else:
        !cp submission_voting.json submission.json

if not is_dry_run and cfg.dataset_path != '/kaggle/input/arc-prize-2024/arc-agi_test_challenges.json':
    print('Results from final submission')
    with open('submission.json', 'r') as f:
        solutions = json.load(f)
    evaluate(data, solutions)
    study_attempt_accuracy(solutions, data)

"""## 7. Clean"""

def clean():
    for filepath in glob.glob('*'):
        if filepath == 'submission.json':
            continue
        if os.path.isdir(filepath):
            shutil.rmtree(filepath)
        else:
            os.remove(filepath)
    !rm -rf /kaggle/tmp/*

clean()
!ls -lh

"""## 8. Show resources usage"""

if not is_dry_run:
    monitor.stop()
    monitor.plot()

"""## References

- https://www.kaggle.com/code/ironbar/fine-tuned-llms-for-arc24-challenge
- https://www.kaggle.com/code/ironbar/fine-tune-llm-on-arc
- https://www.kaggle.com/code/mehrankazeminia/3-arc24-developed-2020-winning-solutions

## TODO

- [x] Can I log metrics when not using wandb? Yes, simply by setting the log to tensorboard
- [x] Combine with icecuber solution
- [x] Group imports
- [ ] Assert that the solution has all the predictions, if there is a timeout there might be missing predictions. It seems that VLLM can get stuck sometimes.
"""